<!-- ---  
layout: page  
title: "EMoG"  
---  


**EMoG:** Synthesizing Emotive Co-speech 3D Gesture with Diffusion Model

**Authors:** Lianying Yin, Yijun Wang, Tianyu He, Jinming Liu, Wei Zhao, Bohan Li, Xin Jin, Jianxin Lin 


**Abstract:** Co-speech 3D human gesture synthesis is a technique employed for generating realistic 3D animations of human motion, using audio signals as input. However, generating highly vivid motion remains a challenge due to the one-to-many nature between the speech content and gestures. This intricate association often leads to the oversight of certain gesture properties, notably the delicate differences of gesture emotion. Moreover, the complexity inherent in audio-driven 3D human gesture synthesis is compounded by the interaction of joint correlation and the temporal variation intrinsic to human gesture, often causing plain gesture generation. In this paper, we present a novel framework, dubbed EMotive Gesture generation (EMoG), to tackle the above challenges with denoising diffusion models: 1) To alleviate the one-to-many problem, we incorporate emotional representations extracted from audio into the distribution modeling process of the diffusion model, explicitly increasing the generation diversity; 2) To enable the emotive gesture generation,  we propose to decompose the difficult gesture generation into two sub-problems: joint correlation modeling and temporal dynamics modeling. Then, the two sub-problems are explicitly tackled with our proposed Temporal-dynamics and Joint-correlation aware Transformer (TJFormer). Through extensive evaluations, we demonstrate that our proposed method surpasses previous state-of-the-art approaches, offering substantial advantages in gesture synthesis. Furthermore, our approach exhibits the additional capabilities of enabling emotion transfer and affording fine-grained style control over specific body parts. This remarkable advancement paves the way for expressive gesture generation with heightened fidelity, opening up new possibilities for immersive experiences in the realm of 3D human gesture synthesis.

**<center>Method</center>**

<figure style="text-align: center;">  
  <img src="assets/images/framework-v4-1.png" alt="Image description" style="transform: scale(0.5);">  
  <figcaption>Framework of Diffcolor. DiffColor mainly contains two stages: 1) colorization with generative color prior that produces accurate and vivid
colorization results; 2) in-context controllable colorization that edits the color of the first-stage output in a way that satisfies the target text prompt.</figcaption>  
</figure>  

**<center>Colorization</center>**


<figure style="text-align: center;">  
  <img src="assets/images/new_main-1.png" alt="Image description" style="transform: scale(0.5);">  
  <figcaption>Different color prompts applied to the same gray image.</figcaption>  
</figure>  

**<center>Comparison</center>**


<figure style="text-align: center;">  
  <img src="assets/images/new_prompt-1.png" alt="Image description" style="transform: scale(0.5);">  
  <figcaption>Comparison with other existing text-conditioned image colorization method.</figcaption>  
</figure>   -->

<!DOCTYPE html>  
<html lang="en">  
<head>  
    <meta charset="UTF-8">  
    <meta name="viewport" content="width=device-width, initial-scale=1.0">  
    <title>EMoG</title>  
    <style>  
        body {  
            font-family: Arial, sans-serif;  
            line-height: 1.6;  
            max-width: 800px;  
            margin: 0 auto;  
            padding: 20px;  
        }  
          
        h1 {  
            font-size: 28px;  
            margin-bottom: 10px;  
            text-align: center;  
        }  
  
        h2 {  
            font-size: 24px;  
            margin-bottom: 10px;  
            text-align: center;  
        }  
          
        figure {  
            margin: 30px 0;  
            text-align: center;  
        }  
          
        img {  
            max-width: 100%;  
            height: auto;  
        }  
          
        figcaption {  
            font-size: 14px;  
            margin-top: 10px;  
        }  
          
        .authors {  
            font-size: 16px;  
            font-weight: bold;  
            text-align: center;  
            margin-bottom: 20px;  
        }  
    </style>  
</head>  
<body>  
  
    <h1>EMoG: Synthesizing Emotive Co-speech 3D Gesture with Diffusion Model</h1>  
  
    <p class="authors">Lianying Yin, Yijun Wang, Tianyu He, Jinming Liu, Wei Zhao, Bohan Li, Xin Jin, Jianxin Lin</p>  
    
  
    <h2> <a href="https://arxiv.org/abs/2306.11496" target="_blank">Paper</a> <a href="https://github.com/1LianYin/EMoG_/tree/main" target="_blank">Code</a> </h2>
  
    <h2>Abstract</h2>
    <p class="abstract">Co-speech 3D human gesture synthesis is a technique employed for generating realistic 3D animations of human motion, using audio signals as input. However, generating highly vivid motion remains a challenge due to the one-to-many nature between the speech content and gestures. This intricate association often leads to the oversight of certain gesture properties, notably the delicate differences of gesture emotion. Moreover, the complexity inherent in audio-driven 3D human gesture synthesis is compounded by the interaction of joint correlation and the temporal variation intrinsic to human gesture, often causing plain gesture generation. In this paper, we present a novel framework, dubbed EMotive Gesture generation (EMoG), to tackle the above challenges with denoising diffusion models: 1) To alleviate the one-to-many problem, we incorporate emotional representations extracted from audio into the distribution modeling process of the diffusion model, explicitly increasing the generation diversity; 2) To enable the emotive gesture generation,  we propose to decompose the difficult gesture generation into two sub-problems: joint correlation modeling and temporal dynamics modeling. Then, the two sub-problems are explicitly tackled with our proposed Temporal-dynamics and Joint-correlation aware Transformer (TJFormer). Through extensive evaluations, we demonstrate that our proposed method surpasses previous state-of-the-art approaches, offering substantial advantages in gesture synthesis. Furthermore, our approach exhibits the additional capabilities of enabling emotion transfer and affording fine-grained style control over specific body parts. This remarkable advancement paves the way for expressive gesture generation with heightened fidelity, opening up new possibilities for immersive experiences in the realm of 3D human gesture synthesis.</p>
    <h2>Method</h2>  
  
    <figure>  
        <img src="assets/images/pipeline.png" alt="Image description">  
        <figcaption>Framework of EMoG. In phase 1, given the raw audio, the initial step involves utilizing the pretrained wav2vec 2.0 model alongside a linear layer to obtain the aligned audio feature referred to as A. Then, a classifier is trained using a supervised manner to distinguish the audio emotion attribute. 
          In phase 2, given the noise gesture sequence x_t, we establish the conditional denoising process in gesture space.</figcaption>  
    </figure>  
  
    <h2>Qualitative Comparison</h2>  
  
    <figure>
      <video width="840" height="600" controls>
        <source src="assets/images/video-comrasion.mp4" type="video/mp4">
      </video>
    </figure>
  
  
    <h2>Emotion Transfer</h2>  
    We perform emotional transfer on the gestures generated by the same audio by specifying emotional labels. Because the original audio has the emotion of fear, the generation results of emotional transfer suggest focusing on the movement of gestures without substituting audio content. The audio is just to prove that the gestures are in sync with the beat of the audio.
    <figure>
      <video width="840" height="600" controls>
        <source src="assets/images/emotion-transfer.mp4" type="video/mp4">
      </video>
    </figure>  
  
</body>  
</html>  
